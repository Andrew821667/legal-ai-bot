"""
RAG (Retrieval-Augmented Generation) Engine
Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ñ‚Ğ°
"""

import logging
import json
import numpy as np
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import config

logger = logging.getLogger(__name__)


class KnowledgeEngine:
    """Ğ”Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²"""
    
    def __init__(self):
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.embedding_model = "text-embedding-3-small"  # Ğ”ĞµÑˆÑ‘Ğ²Ğ°Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        logger.info("KnowledgeEngine initialized")
    
    def get_embedding(self, text: str) -> List[float]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°
        
        Args:
            text: Ğ¢ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°
            
        Returns:
            Ğ’ĞµĞºÑ‚Ğ¾Ñ€ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°
        """
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.embedding_model
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error getting embedding: {e}")
            return []
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸
        
        Args:
            vec1: ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€
            vec2: Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€
            
        Returns:
            ĞšĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (0-1)
        """
        if not vec1 or not vec2:
            return 0.0
        
        vec1_np = np.array(vec1)
        vec2_np = np.array(vec2)
        
        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ
        dot_product = np.dot(vec1_np, vec2_np)
        norm1 = np.linalg.norm(vec1_np)
        norm2 = np.linalg.norm(vec2_np)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def find_similar_conversations(
        self, 
        query: str, 
        conversations: List[Dict],
        top_k: int = 3,
        min_similarity: float = 0.5
    ) -> List[Tuple[Dict, float]]:
        """
        ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ
        
        Args:
            query: Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°
            conversations: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
            top_k: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ğ¿ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
            min_similarity: ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (0-1)
            
        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ¾Ñ€Ñ‚ĞµĞ¶ĞµĞ¹ (Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³, ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾) Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        """
        if not conversations:
            logger.debug("No conversations provided for similarity search")
            return []
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        query_embedding = self.get_embedding(query)
        
        if not query_embedding:
            logger.error("Failed to get query embedding")
            return []
        
        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼
        similarities = []
        
        for conv in conversations:
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°
            conv_text = self._format_conversation_for_embedding(conv)
            conv_embedding = self.get_embedding(conv_text)
            
            if conv_embedding:
                similarity = self.cosine_similarity(query_embedding, conv_embedding)
                
                if similarity >= min_similarity:
                    similarities.append((conv, similarity))
        
        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ¿-K
        top_results = similarities[:top_k]
        
        logger.info(f"Found {len(top_results)} similar conversations (from {len(conversations)} total)")
        for i, (_, sim) in enumerate(top_results, 1):
            logger.debug(f"  #{i}: similarity={sim:.3f}")
        
        return top_results
    
    def _format_conversation_for_embedding(self, conv: Dict) -> str:
        """
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°
        
        Args:
            conv: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°
            
        Returns:
            Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
        """
        # Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸
        if 'messages' in conv:
            messages = conv['messages']
            text_parts = []
            
            for msg in messages[:10]:  # Ğ‘ĞµÑ€Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 10 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
                role = "ĞšĞ»Ğ¸ĞµĞ½Ñ‚" if msg.get('role') == 'user' else "Ğ‘Ğ¾Ñ‚"
                text_parts.append(f"{role}: {msg.get('message', '')}")
            
            text = "\n".join(text_parts)
        
        # Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ´Ğ°
        elif 'pain_point' in conv or 'service_category' in conv:
            parts = []
            
            if conv.get('service_category'):
                parts.append(f"Ğ¢ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°: {conv['service_category']}")
            if conv.get('specific_need'):
                parts.append(f"ĞŸĞ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑŒ: {conv['specific_need']}")
            if conv.get('pain_point'):
                parts.append(f"Ğ‘Ğ¾Ğ»ÑŒ: {conv['pain_point']}")
            if conv.get('industry'):
                parts.append(f"ĞÑ‚Ñ€Ğ°ÑĞ»ÑŒ: {conv['industry']}")
            
            text = " | ".join(parts)
        
        # Ğ˜Ğ½Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ²ÑĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
        else:
            text = " ".join(str(v) for v in conv.values() if v)
        
        return text[:1000]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñƒ
    
    def format_similar_examples_for_prompt(
        self, 
        similar_results: List[Tuple[Dict, float]]
    ) -> str:
        """
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
        
        Args:
            similar_results: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° [(Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³, ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾), ...]
            
        Returns:
            Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
        """
        if not similar_results:
            return ""
        
        examples = []
        
        for i, (conv, similarity) in enumerate(similar_results, 1):
            example_parts = [f"ğŸ“ ĞŸĞ Ğ˜ĞœĞ•Ğ  {i} (Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {similarity:.1%}):"]
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ´Ğ° ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if conv.get('service_category'):
                example_parts.append(f"  â€¢ Ğ¢ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°: {conv['service_category']}")
            if conv.get('specific_need'):
                example_parts.append(f"  â€¢ ĞŸĞ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑŒ: {conv['specific_need']}")
            if conv.get('pain_point'):
                example_parts.append(f"  â€¢ Ğ‘Ğ¾Ğ»ÑŒ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°: {conv['pain_point']}")
            if conv.get('temperature'):
                temp_emoji = {'hot': 'ğŸ”¥', 'warm': 'â™¨ï¸', 'cold': 'â„ï¸'}.get(conv['temperature'], '')
                example_parts.append(f"  â€¢ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: {temp_emoji} {conv['temperature'].upper()} Ğ»Ğ¸Ğ´")
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if conv.get('messages'):
                example_parts.append("  â€¢ Ğ¤Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°:")
                for msg in conv['messages'][:4]:  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 4 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ
                    role = "ĞšĞ»Ğ¸ĞµĞ½Ñ‚" if msg.get('role') == 'user' else "Ğ‘Ğ¾Ñ‚"
                    message = msg.get('message', '')[:150]  # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ
                    example_parts.append(f"    {role}: {message}")
            
            examples.append("\n".join(example_parts))
        
        header = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ«Ğ• ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ˜Ğ— Ğ‘ĞĞ—Ğ« Ğ—ĞĞĞĞ˜Ğ™ (Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        
        footer = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞšĞ¦Ğ˜Ğ¯: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ°Ğº reference Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ¹
ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ• ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾ - Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞ¹ Ğ¿Ğ¾Ğ´ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°,
Ğ½Ğ¾ ÑĞ»ĞµĞ´ÑƒĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        
        return header + "\n\n".join(examples) + "\n" + footer


# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€
knowledge_engine = KnowledgeEngine()


if __name__ == '__main__':
    # Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("Testing KnowledgeEngine...")
    
    # Ğ¢ĞµÑÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°
    test_text = "Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²"
    embedding = knowledge_engine.get_embedding(test_text)
    print(f"Embedding length: {len(embedding)}")
    
    # Ğ¢ĞµÑÑ‚ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°
    test_convos = [
        {
            'service_category': 'Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°',
            'pain_point': 'ĞĞµ ÑƒÑĞ¿ĞµĞ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹',
            'temperature': 'warm'
        },
        {
            'service_category': 'Ğ¡ÑƒĞ´ĞµĞ±Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°',
            'pain_point': 'ĞŸÑ€Ğ¾Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ´ĞµĞ»Ğ°',
            'temperature': 'hot'
        }
    ]
    
    similar = knowledge_engine.find_similar_conversations(
        query="ĞÑƒĞ¶Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ğ²",
        conversations=test_convos,
        top_k=2
    )
    
    print(f"\nFound {len(similar)} similar conversations")
    formatted = knowledge_engine.format_similar_examples_for_prompt(similar)
    print("\nFormatted output:")
    print(formatted)
