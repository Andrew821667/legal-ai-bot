"""
AI Brain - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI GPT + RAG
"""
import logging
from typing import List, Dict, Optional, AsyncGenerator
import json
from openai import OpenAI
import config
import prompts
import database
import knowledge_engine

logger = logging.getLogger(__name__)


class AIBrain:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API"""

    def __init__(self):
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model = config.OPENAI_MODEL
        self.max_tokens = config.MAX_TOKENS
        self.temperature = config.TEMPERATURE

    async def generate_response_stream(self, conversation_history: List[Dict[str, str]]) -> AsyncGenerator[str, None]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π (streaming) –æ—Ç OpenAI

        Args:
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user"/"assistant", "message": "..."}]

        Yields:
            –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ –º–µ—Ä–µ –∏—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI
            messages = [{"role": "system", "content": prompts.SYSTEM_PROMPT}]

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 20 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ–±—Ä—ã–≤–æ–≤
            limited_history = conversation_history[-20:] if len(conversation_history) > 20 else conversation_history
            
            for msg in limited_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg.get("content") or msg.get("message")
                })

            logger.debug(f"Sending streaming request to OpenAI with {len(messages)} messages")

            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º streaming
            # –í–ê–ñ–ù–û: max_completion_tokens = –ª–∏–º–∏—Ç –¢–û–õ–¨–ö–û –Ω–∞ –æ—Ç–≤–µ—Ç (–Ω–µ –≤–∫–ª—é—á–∞–µ—Ç prompt –∏ –∏—Å—Ç–æ—Ä–∏—é!)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_completion_tokens=16000,  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç 16K —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –û–¢–í–ï–¢–ê
                temperature=self.temperature,
                stream=True  # –í–∫–ª—é—á–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—É—é –ø–µ—Ä–µ–¥–∞—á—É!
            )

            # –û—Ç–¥–∞–µ–º —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ä–µ –∏—Ö –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è
            finish_reason = None
            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏—á–∏–Ω—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if chunk.choices[0].finish_reason:
                    finish_reason = chunk.choices[0].finish_reason

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏—á–∏–Ω—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            if finish_reason == "length":
                logger.warning("‚ö†Ô∏è Response was truncated due to max_tokens limit!")
            elif finish_reason == "stop":
                logger.info("‚úì Streaming response completed normally (stop)")
            else:
                logger.info(f"Streaming response completed (finish_reason: {finish_reason})")

        except Exception as e:
            logger.error(f"Error generating streaming response: {e}")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ —Å–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞—à–µ–π –∫–æ–º–∞–Ω–¥–æ–π –Ω–∞–ø—Ä—è–º—É—é."

    def generate_response(self, conversation_history: List[Dict[str, str]]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ + RAG

        Args:
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user"/"assistant", "message": "..."}]

        Returns:
            –û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        """
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI
            messages = [{"role": "system", "content": prompts.SYSTEM_PROMPT}]

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 20 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ–±—Ä—ã–≤–æ–≤
            limited_history = conversation_history[-20:] if len(conversation_history) > 20 else conversation_history
            
            # === RAG: –ò–©–ï–ú –ü–û–•–û–ñ–ò–ï –£–°–ü–ï–®–ù–´–ï –î–ò–ê–õ–û–ì–ò ===
            rag_context = ""
            try:
                # –í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ï–ù–û - —Ç—Ä–µ–±—É–µ—Ç numpy –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
                # TODO: –≤–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ numpy –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
                pass
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
                # last_user_message = next(
                #     (msg['message'] for msg in reversed(limited_history) if msg['role'] == 'user'),
                #     None
                # )
                # 
                # if last_user_message and len(last_user_message) > 10:
                #     # –ü–æ–ª—É—á–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –∏–∑ –ë–î
                #     successful_convos = database.db.get_successful_conversations(limit=30)
                #     
                #     if successful_convos:
                #         # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
                #         similar = knowledge_engine.knowledge_engine.find_similar_conversations(
                #             query=last_user_message,
                #             conversations=successful_convos,
                #             top_k=2,  # –¢–æ–ø-2 –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–∏–º–µ—Ä–∞
                #             min_similarity=0.6  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ 60%
                #         )
                #         
                #         if similar:
                #             # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
                #             rag_context = knowledge_engine.knowledge_engine.format_similar_examples_for_prompt(similar)
                #             logger.info(f"üìö RAG: Found {len(similar)} similar conversations, adding to context")
            
            except Exception as e:
                logger.warning(f"RAG search failed (non-critical): {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ RAG –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
            
            # –î–û–ë–ê–í–õ–Ø–ï–ú RAG –ö–û–ù–¢–ï–ö–°–¢ –ï–°–õ–ò –ù–ê–®–õ–ò
            if rag_context:
                messages.append({
                    "role": "system",
                    "content": rag_context
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
            for msg in limited_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg.get("content") or msg.get("message")
                })

            logger.debug(f"Sending request to OpenAI with {len(messages)} messages (RAG: {bool(rag_context)})")

            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI
            # –í–ê–ñ–ù–û: max_completion_tokens = –ª–∏–º–∏—Ç –¢–û–õ–¨–ö–û –Ω–∞ –æ—Ç–≤–µ—Ç (–Ω–µ –≤–∫–ª—é—á–∞–µ—Ç prompt!)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_completion_tokens=16000,  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç 16K —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –û–¢–í–ï–¢–ê
                temperature=self.temperature
            )

            assistant_message = response.choices[0].message.content
            finish_reason = response.choices[0].finish_reason

            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω
            if finish_reason == "length":
                logger.warning(f"‚ö†Ô∏è Response truncated! ({len(assistant_message)} chars, finish_reason: length)")
            else:
                logger.info(f"Received response from OpenAI: {len(assistant_message)} chars (finish_reason: {finish_reason}, RAG: {bool(rag_context)})")

            return assistant_message

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ —Å–≤—è–∂–∏—Ç–µ—Å—å —Å –ê–Ω–¥—Ä–µ–µ–º –Ω–∞–ø—Ä—è–º—É—é."

    def extract_lead_data(self, conversation_history: List[Dict[str, str]]) -> Optional[Dict]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ª–∏–¥–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞

        Args:
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ª–∏–¥–∞ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
            conversation_text = "\n".join([
                f"{msg['role']}: {msg['message']}"
                for msg in conversation_history
            ])

            messages = [
                {"role": "system", "content": prompts.EXTRACT_DATA_PROMPT},
                {"role": "user", "content": f"–î–∏–∞–ª–æ–≥:\n{conversation_text}"}
            ]

            logger.debug("Extracting lead data from conversation")

            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=500,
                temperature=0.3  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            )

            response_text = response.choices[0].message.content
            logger.debug(f"Received extraction response: {response_text[:100]}")

            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ markdown –±–ª–æ–∫–∏
            response_text = response_text.strip()
            if response_text.startswith("```"):
                # –£–±–∏—Ä–∞–µ–º markdown –æ–±–µ—Ä—Ç–∫—É
                lines = response_text.split("\n")
                response_text = "\n".join(lines[1:-1])

            lead_data = json.loads(response_text)
            
            # –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.info(f"‚úÖ Lead data extracted: temperature={lead_data.get('lead_temperature')}")
            logger.info(f"üìä Service: category={lead_data.get('service_category')}, need={lead_data.get('specific_need')}")
            logger.info(f"üîç Full lead data: {json.dumps(lead_data, ensure_ascii=False)}")

            return lead_data

        except json.JSONDecodeError as e:
            logger.error(f"Error parsing JSON response: {e}, response: {response_text}")
            return None
        except Exception as e:
            logger.error(f"Error extracting lead data: {e}")
            return None

    def check_handoff_trigger(self, user_message: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥–∞—á–∏ –∞–¥–º–∏–Ω—É

        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∞–¥–º–∏–Ω—É
        """
        message_lower = user_message.lower()

        for trigger in prompts.HANDOFF_TRIGGERS:
            if trigger.lower() in message_lower:
                logger.info(f"Handoff trigger detected: {trigger}")
                return True

        return False

    def should_offer_lead_magnet(self, lead_data: Optional[Dict]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å lead magnet

        Args:
            lead_data: –î–∞–Ω–Ω—ã–µ –ª–∏–¥–∞

        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å lead magnet
        """
        if not lead_data:
            return False

        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º lead magnet –µ—Å–ª–∏:
        # 1. –ï—Å—Ç—å –±–æ–ª—å
        # 2. –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–æ–Ω—Ç–∞–∫—Ç (email –∏–ª–∏ phone) –ò–õ–ò
        # 3. –õ–∏–¥ —Ç–µ–ø–ª—ã–π –∏–ª–∏ –≥–æ—Ä—è—á–∏–π

        has_pain = lead_data.get('pain_point') and len(lead_data.get('pain_point', '')) > 10
        has_contact = lead_data.get('email') or lead_data.get('phone')
        temperature = lead_data.get('lead_temperature', 'cold')

        should_offer = has_pain and (has_contact or temperature in ['warm', 'hot'])

        logger.debug(f"Should offer lead magnet: {should_offer} (pain={has_pain}, contact={has_contact}, temp={temperature})")

        return should_offer


# –°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
ai_brain = AIBrain()


if __name__ == '__main__':
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("Testing AIBrain...")

    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
    test_conversation = [
        {"role": "user", "message": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤"}
    ]

    response = ai_brain.generate_response(test_conversation)
    print(f"\nResponse: {response[:200]}...")

    # –¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    test_conversation_full = [
        {"role": "user", "message": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤"},
        {"role": "assistant", "message": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ –≤–∞—à–µ–π –∫–æ–º–∞–Ω–¥–µ"},
        {"role": "user", "message": "–£ –Ω–∞—Å 5 —é—Ä–∏—Å—Ç–æ–≤, –æ–∫–æ–ª–æ 50 –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –≤ –º–µ—Å—è—Ü"},
        {"role": "assistant", "message": "–ü–æ–Ω—è—Ç–Ω–æ. –ö–∞–∫–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞?"},
        {"role": "user", "message": "–ù–µ —É—Å–ø–µ–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å –≤—Å–µ, –∏–Ω–æ–≥–¥–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã. –ë—é–¥–∂–µ—Ç –¥–æ 500 —Ç—ã—Å—è—á. –ú–æ–π email: ivan@company.ru"}
    ]

    lead_data = ai_brain.extract_lead_data(test_conversation_full)
    print(f"\nExtracted lead data: {json.dumps(lead_data, indent=2, ensure_ascii=False)}")
